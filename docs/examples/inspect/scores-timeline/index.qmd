---
title: "Scores Timeline"
subtitle: "Data: [evals.parquet](evals.parquet)"
resources:
   - import.py
---

This example illustrates the code behind the [`scores_timeline()`](../../../view-scores-timeline.qmd) pre-built view function. If you want to include this plot in your notebooks or websites you should start with that function rather than the lower-level code below.

We plot several benchmarks against models from various organizations, with release date on the x-axis. The original data for this plot was published by the [Epoch AI Benchmarking Hub](https://epoch.ai/data/ai-benchmarking-dashboard). The fields in the dataset are as follows:

- `model`: Model name (e.g. "gpt-4o")
- `organization`: Organization that created the model (e.g. "OpenAI")
- `release_date`: Date of model release.
- `eval`: Name of eval (e.g. "SWE-bench Verified")
- `scorer`: Scorer used (e.g. "choice").
- `score`: Benchmark score (scaled 0-1).
- `stderr`: Standard error.
- `log_viewer`: Optional. URL to view evaluation log.

Here is the plot and code required to produce it (click on the numbers at right for further explanation):

```{python}
#| filename: Code

from inspect_viz import Data, Selection
from inspect_viz.input import checkbox_group, select
from inspect_viz.layout import vconcat, vspace
from inspect_viz.plot import plot, legend
from inspect_viz.mark import dot, rule_x
from inspect_viz.table import table
from inspect_viz.transform import ci_bounds

# read data
evals = Data.from_file("evals.parquet") # <1>

# transforms to compute ci bounds from score and stderr columns
ci_lower, ci_upper = ci_bounds(0.95, score="score", stderr="stderr") # <2>

vconcat(
    # select benchmark
    select(evals, label="Eval: ", column="eval", value="GPQA Diamond", width=425),
    
    # filter models by organization(s)
    checkbox_group(evals, column="organization"),
    
    # dot plot w/ error bars
    vspace(15),
    plot(
        dot(
            evals,
            x="release_date",
            y="score",
            r=3,
            fill="organization",
            channels= {  # <3>
                "Model": "model", 
                "Scorer": "scorer", 
                "Stderr": "stderr",
                "Log Viewer": "log_viewer"
            } # <3>
        ),
        rule_x( 
            evals,
            x="release_date",
            y="score",
            y1=ci_lower,
            y2=ci_upper,
            stroke="organization",
            stroke_opacity=0.4, # <4>
            marker="tick-x",
        ), 
        legend=legend("color", target=evals.selection), # <5>
        x_domain="fixed",  # <6>
        y_domain=[0,1.0],  # <6>
        x_label="Release Date",
        y_label="Score",
        color_label="Organization"
    )
)
```

1. Benchmark data sourced from [Epoch AI](https://epoch.ai/data/ai-benchmarking-dashboard) and prepared with [import.py](import.py).

2. Create transforms used to compute the confidence intervals for each point.

3. Additional channels are added to the tooltip.

4. Confidence interval: compute dynamically using `ci_value()`, color by organization, and reduce opacity.

5. Specifying `target` makes the legend clickable.

6. Domains: `x_domain` fixed so that the axes don't jump around for organization selections; `y_domain` should always span up to 1.0.



