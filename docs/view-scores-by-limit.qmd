---
title: "Scores by Limit"
reference: "inspect_viz.view.beta"
filters:
   - at: pre-ast
     type: json
     path: reference/filter/filter.py
   - at: pre-ast
     path: reference/filter/post.lua
datafile: "swebench_token_limit.parquet"
---

## Overview

The `scores_by_limit()` function renders a line plot for evaluating how a model's **success rate** changes as the **compute budget** increases (e.g., token limit or time). It helps answer “*Will performance keep improving if I spend more?*”. The shaded band displays the confidence interval derived from the standard error. 

This visualization requires that you run your evaluation wiht a very high time or token limit, allowing the model a large amount of the resource to complete each sample in the evaluation. Then, use the  `scores_by_limit_df()` function will to prepare the dataframe for visualization, computing the amount of the time or tokens required to solve each sample.

```{python}
from inspect_viz import Data
from inspect_viz.view.beta import scores_by_limit

evals = Data.from_file("swebench_token_limit.parquet")
scores_by_limit(evals)
```

## Data Preparation

{{< include _data_preparation_scores_by_limit.md >}}

{{< include _data_preparation_optional.md >}}

## Function Reference

### scores_by_limit {reference="scores_by_limit"}

### scores_by_limit_df {reference="scores_by_limit_df"}

## Implementation

The [Scores by Limit](examples/inspect/scores-by-limit/index.qmd) example demonstrates how this view was implemented using lower level plotting components.
